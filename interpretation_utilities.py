import logomaker
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.stats import chisquare
import torch

from data import PandasDataset

def calculate_entropy(pwms):
    """
    Calculate the entropies of the position weight matrices.
    """
    return -np.sum(pwms * np.log2(pwms+1e-7), axis=(1,2))


def get_consensus_sequence(pwm, char_vocab_inv_map):
    """
    Calculate the consensus sequence of a position weight matrix using the
    plurality of A, C, G, N, or T.
    """
    return "".join([char_vocab_inv_map[i] for i in np.argmax(pwm, axis=0)])


def get_pwms(seqs, fmap, window, c, top_k):
    """
    Returns position weight matrices and their entropies corresponding to the sequences that activate each filter.
    
    Keyword arguments
    seqs   --    torch.Tensor of batched sequences
    fmap   --    torch.Tensor of feature maps generated by convolution
    window --    int of window length for position weight matrices
    c      --    float of constant to scale activations before exponentiation
    top_k  --    int of number of top activations to use to generate position weight matrices
    
    Position weight matrices are generated by a weighted average of sequences by their
    activation scores.
    """
    window_left = int(window/2)
    window_right = window - window_left
    
    num_seqs = fmap.shape[0]
    num_filters = fmap.shape[1]
    seq_length = fmap.shape[2]
    
    pwms = []
    
    for filter_idx in range(num_filters):   
        # Sort by decreasing score (take the top k activations)
        data_idx, pos_idx = np.unravel_index(
            np.argsort(fmap[:, filter_idx, :], axis=None)[::-1][:top_k], 
            fmap[:, filter_idx, :].shape
        )
        
        # Make a sequence alignment centered about each activation (above threshold)
        seq_align = []
        for i in range(len(pos_idx)):
            start_window = pos_idx[i] - window_left
            end_window = pos_idx[i] + window_right
            # Ensure positions are valid
            if start_window >= 0 and end_window <= seq_length:
                seq = seqs[data_idx[i],:,start_window:end_window]
                # Compute score to add to sequence alignment depending on relative strength
                weighted_seq = seq * fmap[data_idx[i], filter_idx, pos_idx[i]]
                seq_align.append(weighted_seq)
        
        avg_seq_align = np.mean(np.array(seq_align), axis=0)
        normed_seq_align = avg_seq_align / np.linalg.norm(avg_seq_align, ord=1, axis=0)
        pwms.append(normed_seq_align)
            
    pwms = np.array(pwms)
    entropies = calculate_entropy(pwms)
    return pwms, entropies


def get_MEME_string(pwms, frequencies, window):
    """
    Create the MEME-formatted string for a set of position weight matrices.
    """
    
    # Renormalize frequencies
    freq_A = frequencies["A"]
    freq_C = frequencies["C"]
    freq_G = frequencies["G"]
    freq_T = frequencies["T"]
    freq_sum = freq_A + freq_C + freq_G + freq_T
    
    freq_A = freq_A / freq_sum
    freq_C = freq_C / freq_sum
    freq_G = freq_G / freq_sum
    freq_T = freq_T / freq_sum
    
    meme_string = f"""MEME version 4
    
ALPHABET = ACGT

strands: + -

Background letter frequencies
A {freq_A} C {freq_C} G {freq_G} T {freq_T} 

"""
    

    for idx in range(len(pwms)):
        identifier = f"{idx + 1}"
        
        try:
            acgt_pwm = pwms[idx][[0, 1, 2, 4], :] / np.linalg.norm(pwms[idx][[0, 1, 2, 4], :], ord=1, axis=0)
        
            matrix = np.array2string(
                acgt_pwm.T, 
                separator="  ", 
                edgeitems=np.inf, 
                suppress_small=True, 
                precision=5,
                floatmode="fixed",
            ).replace("]","").replace("[[", "").replace(" [","")
    
            meme_string += f"""MOTIF {identifier}
letter-probability matrix: alength= 4 w={window}
{matrix}

"""
        except RuntimeWarning:
            # Ignore motifs with nans caused by div-zero errors
            continue
        
    return meme_string


def plot_promoter_filters(filters, feature_maps, percentile_cutoff, selection=None, entropies=None):
    NUM_ROWS = 8
    NUM_COLS = 2
    
    num_filter, A, filter_len = filters.shape
    if selection is None:
        selection = [*range(num_filter)]
    
    fig, axs = plt.subplots(ncols=NUM_COLS * 2, nrows=NUM_ROWS, figsize=(11, 8.5), constrained_layout=True)
    
    for idx, (n, f) in enumerate(zip(selection, filters[selection])):
        row = idx // NUM_COLS
        col = idx % NUM_COLS
            
        # Calculate sequence logo heights -- information
        # Excerpt from MEME Suite ceqlogo.c source code
        """
        The letter stacks are calculated by the following equations found in
        Schneider and Stephens paper "Sequence Logos: A New Way to Display
        Consensus Sequences":

         height = f(b,l) * R(l)                            (1)

        where f(b,l) is the frequency of base or amino acid "b" at position
        "l". R(l) is amount of information present at position "l" and can
        be quantified as follows:

         R(l) for amino acids   = log(20) - (H(l) + e(n))    (2a)
         R(l) for nucleic acids =    2    - (H(l) + e(n))    (2b)

        where log is taken base 2, H(l) is the uncertainty at position "l",
        and e(n) is the error correction factor for small "n". H(l) is
        computed as follows:

           H(l) = - (Sum f(b,l) * log[ f(b,l) ])             (3)

        where again, log is taken base 2. f(b,l) is the frequency of base
        "b" at position "l". The sum is taken over all amino acids or
        bases, depending on which the data is.

        Currently, logo.pm uses an approximation for e(n), given by:

           e(n) = (s-1) / (2 * ln2 * n)                      (4)

        Where s is 4 for nucleotides, 20 for amino acids ; n is the number
        of sequences in the alignment. e(n) also  gives the height of error
        bars.
        """
        e = 0 # No error correction
        
        H = -np.sum(f * np.log2(f+1e-7), axis=0, keepdims=True)
        R = np.log2(5) - (H + e)
        logo = f * R

        # Create DataFrame for logomaker
        counts_df = pd.DataFrame(data=0.0, columns=list('ACGNT'), index=list(range(9)))
        for a in range(A):
            for l in range(filter_len):
                counts_df.iloc[l,a] = logo[a,l]

        # Plot filter representation
        logo = logomaker.Logo(counts_df, 
                              color_scheme="classic",
                              ax=axs[row, col * 2])
        logo.style_spines(visible=False)
        logo.style_spines(spines=["left", "bottom"], visible=True)
        logo.style_xticks(rotation=90, fmt='%d', anchor=0)

        
        logo.ax.xaxis.set_ticks_position('none')
        logo.ax.xaxis.set_tick_params(pad=-1)
        
        axs[row, col * 2].set(title=f"SC Motif #{n + 1}; Entropy: {entropies[idx]:.3f}",
                              xticks=[i for i in range(9)],
                              xticklabels=[i + 1 for i in range(9)],
                              ylim=(0, 2.2),
                              ylabel="bits")
        
        # Calculate plot of top activation distributions over sequence        
        cutoff = np.percentile(feature_maps[:, n, :], percentile_cutoff)
        transformed_fmap = feature_maps.copy()
        transformed_fmap[transformed_fmap < cutoff] = np.nan
        
        # Also do a zero filter
        transformed_fmap[transformed_fmap == 0] = np.nan

        feature_length = feature_maps.shape[2]
        x_range = [i for i in reversed(range(feature_length))]

        counts = np.sum(~np.isnan(transformed_fmap[:, n, :]), axis=0)
                
        smoothk = 15
        smoothed_counts = torch.nn.functional.avg_pool1d(
            torch.from_numpy(counts).unsqueeze(0).unsqueeze(0),
            kernel_size=(smoothk,),
            stride=(1,),
            padding=(smoothk//2,),
            count_include_pad=False
        ).view(-1).numpy()
        
        axs[row, col * 2 + 1].plot(x_range, smoothed_counts)
        axs[row, col * 2 + 1].set(xlim=(0, feature_length), ylim=(0, 1.1 * np.max(smoothed_counts[10:feature_length-10])))
        axs[row, col * 2 + 1].invert_xaxis()
    return fig


def get_top_sequences(fmap, kernel, dataset, percentile):
    highest_sequence_activations = np.amax(fmap[:, kernel, :], axis=1)
    cutoff = np.percentile(highest_sequence_activations, percentile)
    
    return [dataset[index]["id"] 
            for index in np.argwhere(highest_sequence_activations >= cutoff).flatten().tolist()]


def get_chisquare_uniform(fmap, kernel, percentile_marker):
    cutoff = np.percentile(fmap[:, kernel, :], percentile_marker)
    transformed_fmap = fmap[:, kernel, 10:990].copy()
    transformed_fmap[transformed_fmap < cutoff] = np.nan

    counts = np.sum(~np.isnan(transformed_fmap), axis=0)
    windowed_counts = counts.reshape((49, 20)).sum(axis=1)

    return chisquare(windowed_counts).statistic


def activation_cutoff(fmap, kernel, percentile_marker):
    cutoff = np.percentile(fmap[:, kernel, :], percentile_marker)
    return cutoff