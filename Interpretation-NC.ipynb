{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from data import JsonDataset, get_samplers\n",
    "from data import domain, split_data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logomaker\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "   \"learning_rate\": 0.00010229218879330196,\n",
    "   \"weight_decay\": 0.0016447149582678627,\n",
    "   \"xt_hidden\": 1024,\n",
    "   \"seq_length\": 1000,\n",
    "   \"conv1_ksize\": 9,\n",
    "   \"conv1_knum\": 256,\n",
    "   \"pool1\": 19,\n",
    "   \"conv2_ksize\": 13,\n",
    "   \"conv2_knum\": 64,\n",
    "   \"pool2\": 8,\n",
    "   \"batch_size\": 256,\n",
    "   \"conv_activation\" : \"relu\",\n",
    "   \"fc_activation\" : \"elu\",\n",
    "   \"conv_dropout\": 0.3981796388676127,\n",
    "   \"tf_dropout\": 0.18859739941162465,\n",
    "   \"fc_dropout\": 0.016570328292903613,\n",
    "   \"use_cuda\": USE_CUDA,\n",
    "   \"prefix\":'nc_elem_',\n",
    "   \"num_tfs\":325\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu\n"
     ]
    }
   ],
   "source": [
    "if USE_CUDA:\n",
    "    print(\"using gpu\")\n",
    "    cuda = torch.device('cuda')\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "    def cudaify(model):\n",
    "        return model.cuda()\n",
    "else: \n",
    "    print(\"using cpu\")\n",
    "    cuda = torch.device('cpu')\n",
    "    FloatTensor = torch.FloatTensor\n",
    "    LongTensor = torch.LongTensor\n",
    "    def cudaify(model):\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"\n",
    "    A simple vocabulary class that takes an alphabet of symbols,\n",
    "    and assigns each symbol a unique integer id., e.g.\n",
    "    \n",
    "    > v = Vocab(['a','b','c','d'])\n",
    "    > v('c')\n",
    "    2\n",
    "    > v('a')\n",
    "    0\n",
    "    > len(v)\n",
    "    4\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, alphabet):\n",
    "        self.alphabet = alphabet\n",
    "        self.index_map = {letter: index for (index, letter) in \n",
    "                          list(enumerate(alphabet))}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.alphabet)\n",
    "        \n",
    "    def __call__(self, letter):\n",
    "        return self.index_map[letter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(Dataset):\n",
    "\n",
    "    def __init__(self, pd_file):\n",
    "        self.data = pd.read_pickle(pd_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.loc[idx].to_dict()\n",
    "\n",
    "    def select(self, field):\n",
    "        for i, datum in self.data.iterrows():\n",
    "            yield datum[field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensorize:\n",
    "    \"\"\"\n",
    "    An instance of Tensorize is a function that maps a piece of data\n",
    "    (i.e. a dictionary) to an input and output tensor for consumption by\n",
    "    a neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, symbol_vocab, max_word_length):\n",
    "        self.symbol_vocab = symbol_vocab\n",
    "        self.max_word_length = max_word_length\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        words = Tensorize.words_to_tensor(data['seq'], \n",
    "                                              self.symbol_vocab, \n",
    "                                              self.max_word_length).float()\n",
    "        \n",
    "        tfs = torch.stack(data['tfs'],dim=1).float()\n",
    "        label = data['exp'].float()\n",
    "        return cudaify(words), cudaify(tfs), cudaify(label)\n",
    "        \n",
    "    @staticmethod\n",
    "    def words_to_tensor(words, vocab, max_word_length):\n",
    "        \"\"\"\n",
    "        Turns an K-length list of words into a <K, len(vocab), max_word_length>\n",
    "        tensor.\n",
    "    \n",
    "        e.g.\n",
    "            t = words_to_tensor(['BAD', 'GAB'], Vocab('ABCDEFG'), 3)\n",
    "            # t[0] is a matrix representations of 'BAD', where the jth\n",
    "            # column is a one-hot vector for the jth letter\n",
    "            print(t[0])\n",
    "    \n",
    "        \"\"\"\n",
    "        tensor = torch.zeros(len(words), len(vocab), max_word_length)\n",
    "        for i, word in enumerate(words):\n",
    "            start_index = max(0, len(word) - max_word_length)\n",
    "            for li, letter in enumerate(word[start_index:len(word)][::-1]):\n",
    "                tensor[i][vocab(letter)][max_word_length - li - 1] = 1\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProCNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A two layer CNN that uses max pooling to extract information from\n",
    "    the sequence input, a one layer fully-connected network to extract TF information \n",
    "    then trains a fully connected neural network on the to make predictions.\n",
    "    \n",
    "    \"\"\"    \n",
    "    def __init__(self, config, output_classes, input_symbol_vocab):\n",
    "        super(ProCNN, self).__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.input_symbol_vocab = input_symbol_vocab\n",
    "        \n",
    "        self.xt_hidden = config[\"xt_hidden\"]\n",
    "        self.xs_hidden = config[\"seq_length\"] // (config[\"pool1\"] * config[\"pool2\"])\n",
    "\n",
    "        self.output_classes = output_classes\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(len(input_symbol_vocab),config[\"conv1_knum\"], kernel_size=config[\"conv1_ksize\"], stride=1, padding=config[\"conv1_ksize\"]//2)\n",
    "        self.pool1 = torch.nn.MaxPool1d(kernel_size=config[\"pool1\"], stride=config[\"pool1\"])\n",
    "        self.activation1 = self.get_activation(config[\"conv_activation\"])\n",
    "        self.conv1dropout = torch.nn.Dropout(config[\"conv_dropout\"])\n",
    "        self.conv1bn = torch.nn.BatchNorm1d(config[\"conv1_knum\"])\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv1d(config[\"conv1_knum\"],config[\"conv2_knum\"], kernel_size=config[\"conv2_ksize\"], stride=1, padding=config[\"conv2_ksize\"]//2)\n",
    "        self.pool2 = torch.nn.MaxPool1d(kernel_size=config[\"pool2\"], stride=config[\"pool2\"])\n",
    "        self.activation2 = self.get_activation(config[\"conv_activation\"])\n",
    "        self.conv2dropout = torch.nn.Dropout(config[\"conv_dropout\"])\n",
    "        self.conv2bn = torch.nn.BatchNorm1d(config[\"conv2_knum\"])\n",
    "        \n",
    "        self.fc_t = torch.nn.Linear(config[\"num_tfs\"],self.xt_hidden)\n",
    "        self.fc_t_activation = self.get_activation(config[\"fc_activation\"])\n",
    "        self.fc_t_dropout = torch.nn.Dropout(config[\"fc_dropout\"])\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(config[\"conv2_knum\"]*self.xs_hidden+self.xt_hidden, 512)\n",
    "        self.fc1_activation = self.get_activation(config[\"fc_activation\"])\n",
    "        self.fc1_dropout = torch.nn.Dropout(config[\"fc_dropout\"])\n",
    "        self.bn1 = torch.nn.BatchNorm1d(num_features=512)\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(512, 256)\n",
    "        self.fc2_activation = self.get_activation(config[\"fc_activation\"])\n",
    "        self.fc2_dropout = torch.nn.Dropout(config[\"fc_dropout\"])\n",
    "        self.bn2 = torch.nn.BatchNorm1d(num_features=256)\n",
    "        \n",
    "        self.fc3 = torch.nn.Linear(256, 64)\n",
    "        self.fc3_activation = self.get_activation(config[\"fc_activation\"])\n",
    "        self.fc3_dropout = torch.nn.Dropout(config[\"fc_dropout\"])\n",
    "        self.bn3 = torch.nn.BatchNorm1d(num_features=64)\n",
    "        \n",
    "        self.fc_out = torch.nn.Linear(64,1)\n",
    "        \n",
    "    @classmethod\n",
    "    def get_activation(cls, activation):\n",
    "        if activation == \"relu\":\n",
    "            return torch.nn.ReLU()\n",
    "        elif activation == \"gelu\":\n",
    "            return torch.nn.GELU()\n",
    "        elif activation == \"elu\":\n",
    "            return torch.nn.ELU()\n",
    "        elif activation == \"selu\":\n",
    "            return torch.nn.SELU()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function: must be relu, gelu, or elu.\")\n",
    "    \n",
    "    def get_input_vocab(self):\n",
    "        return self.input_symbol_vocab\n",
    "    \n",
    "    def forward(self, x_s, x_t):\n",
    "        b=list(x_s.size())[0] #batch size\n",
    "        x_s = self.conv1(x_s)\n",
    "        x_s = self.pool1(x_s)\n",
    "        x_s = self.activation1(x_s)\n",
    "        x_s = self.conv1dropout(x_s)\n",
    "        x_s = self.conv1bn(x_s)\n",
    "        \n",
    "        x_s = self.conv2(x_s)\n",
    "        x_s = self.pool2(x_s)\n",
    "        x_s = self.activation2(x_s)\n",
    "        x_s = self.conv2dropout(x_s)\n",
    "        x_s = self.conv2bn(x_s)\n",
    "        \n",
    "        x_t = self.fc_t(x_t)\n",
    "        x_t = self.fc_t_activation(x_t)\n",
    "        x_t = self.fc_t_dropout(x_t)\n",
    "        \n",
    "        x = torch.cat((x_s.reshape((b,self.config[\"conv2_knum\"]*self.xs_hidden,1)).view(-1, self.config[\"conv2_knum\"]*self.xs_hidden),x_t),1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1_activation(x)\n",
    "        x = self.fc1_dropout(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2_activation(x)\n",
    "        x = self.fc2_dropout(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.fc3_activation(x)\n",
    "        x = self.fc3_dropout(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PandasDataset('Data_pkl/file_oav_filt05_filtcv3_Zlog_new_allUniqueGenes.pkl')\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=64, num_workers=8, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab = Vocab(['A', 'C', 'G', 'N', 'T'])    \n",
    "tens = Tensorize(char_vocab, CONFIG[\"seq_length\"])\n",
    "char_vocab.index_map\n",
    "char_vocab_inv_map = {v: k for k, v in char_vocab.index_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nucleotide_alphabet(dataset):\n",
    "    symbols = set()\n",
    "    all_symbols = []\n",
    "    for sequence in dataset.select('seq'):\n",
    "        letters = set(sequence)\n",
    "        symbols = symbols | letters\n",
    "        all_symbols += list(sequence)\n",
    "        \n",
    "    freqs = dict()\n",
    "    for sym in symbols:\n",
    "        freqs[sym] = sum([1 for i in all_symbols if i == sym]) / len(all_symbols)\n",
    "        \n",
    "    return sorted(list(symbols)), freqs\n",
    "\n",
    "_, frequencies = extract_nucleotide_alphabet(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProCNN(\n",
       "  (conv1): Conv1d(5, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "  (pool1): MaxPool1d(kernel_size=19, stride=19, padding=0, dilation=1, ceil_mode=False)\n",
       "  (activation1): ReLU()\n",
       "  (conv1dropout): Dropout(p=0.3981796388676127, inplace=False)\n",
       "  (conv1bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(256, 64, kernel_size=(13,), stride=(1,), padding=(6,))\n",
       "  (pool2): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "  (activation2): ReLU()\n",
       "  (conv2dropout): Dropout(p=0.3981796388676127, inplace=False)\n",
       "  (conv2bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc_t): Linear(in_features=325, out_features=1024, bias=True)\n",
       "  (fc_t_activation): ELU(alpha=1.0)\n",
       "  (fc_t_dropout): Dropout(p=0.016570328292903613, inplace=False)\n",
       "  (fc1): Linear(in_features=1408, out_features=512, bias=True)\n",
       "  (fc1_activation): ELU(alpha=1.0)\n",
       "  (fc1_dropout): Dropout(p=0.016570328292903613, inplace=False)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2_activation): ELU(alpha=1.0)\n",
       "  (fc2_dropout): Dropout(p=0.016570328292903613, inplace=False)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc3_activation): ELU(alpha=1.0)\n",
       "  (fc3_dropout): Dropout(p=0.016570328292903613, inplace=False)\n",
       "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_state = torch.load('nc_elem_best_net/checkpoint.pt')\n",
    "model = ProCNN(CONFIG, 1, char_vocab)\n",
    "model.load_state_dict(checkpoint_state[\"model_state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(5, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
       "  (1): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_conv = torch.nn.Sequential(*list(model.children())[:1]+[torch.nn.ReLU()])\n",
    "cnn_conv = cudaify(cnn_conv)\n",
    "cnn_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_conv.eval()\n",
    "fmap = []\n",
    "x_test = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(loader):\n",
    "        input_s, _, _ = tens(data)\n",
    "        fmap.append(cnn_conv(input_s).to(\"cpu\"))\n",
    "        x_test.append(input_s.to(\"cpu\"))\n",
    "            \n",
    "fmap = np.concatenate(fmap)\n",
    "x_test = np.concatenate(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5681, 256, 1000) (5681, 5, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(fmap.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fmap, f\"nc-elem-promoters-fmap.pt\", pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate PWMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(pwms):\n",
    "    return -np.sum(pwms * np.log2(pwms+1e-7), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consensus_sequence(pwm):\n",
    "    return \"\".join([char_vocab_inv_map[i] for i in np.argmax(pwm, axis=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pwms(seqs, fmap, window, c, top_k):\n",
    "    \"\"\"\n",
    "    Returns position-weight-matrices and their entropies corresponding to the sequences that activate each filter.\n",
    "    \n",
    "    Keyword arguments\n",
    "    seqs   --    torch.Tensor of batched sequences\n",
    "    fmap   --    torch.Tensor of feature maps generated by convolution\n",
    "    window --    int of window length for position weight matrices\n",
    "    c      --    float of constant to scale activations before exponentiation\n",
    "    top_k  --    int of number of top activations to use to generate position weight matrices\n",
    "    \n",
    "    Position weight matrices are generated by a weighted average of sequences by their\n",
    "    activation scores.\n",
    "    \"\"\"\n",
    "    window_left = int(window/2)\n",
    "    window_right = window - window_left\n",
    "    \n",
    "    num_seqs = fmap.shape[0]\n",
    "    num_filters = fmap.shape[1]\n",
    "    seq_length = fmap.shape[2]\n",
    "    \n",
    "    pwms = []\n",
    "    entropies = []\n",
    "    \n",
    "    for filter_idx in range(num_filters):   \n",
    "        # Sort by decreasing score (take the top k activations)\n",
    "        data_idx, pos_idx = np.unravel_index(\n",
    "            np.argsort(fmap[:, filter_idx, :], axis=None)[::-1][:top_k], \n",
    "            fmap[:, filter_idx, :].shape\n",
    "        )\n",
    "        \n",
    "        # Make a sequence alignment centered about each activation (above threshold)\n",
    "        seq_align = []\n",
    "        for i in range(len(pos_idx)):\n",
    "            start_window = pos_idx[i] - window_left\n",
    "            end_window = pos_idx[i] + window_right\n",
    "            # Ensure positions are valid\n",
    "            if start_window >= 0 and end_window <= seq_length:\n",
    "                seq = seqs[data_idx[i],:,start_window:end_window]\n",
    "                # Compute score to add to sequence alignment depending on relative strength\n",
    "                weighted_seq = seq * fmap[data_idx[i], filter_idx, pos_idx[i]]\n",
    "                seq_align.append(weighted_seq)\n",
    "        \n",
    "        avg_seq_align = np.mean(np.array(seq_align), axis=0)\n",
    "        normed_seq_align = avg_seq_align / np.linalg.norm(avg_seq_align, ord=1, axis=0)\n",
    "        pwms.append(normed_seq_align)\n",
    "            \n",
    "    pwms = np.array(pwms)\n",
    "    entropies = calculate_entropy(pwms)\n",
    "    return pwms, entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MEME_string(pwms, frequencies, window):\n",
    "    \n",
    "    # Renormalize frequencies\n",
    "    freq_A = frequencies[\"A\"]\n",
    "    freq_C = frequencies[\"C\"]\n",
    "    freq_G = frequencies[\"G\"]\n",
    "    freq_T = frequencies[\"T\"]\n",
    "    freq_sum = freq_A + freq_C + freq_G + freq_T\n",
    "    \n",
    "    freq_A = freq_A / freq_sum\n",
    "    freq_C = freq_C / freq_sum\n",
    "    freq_G = freq_G / freq_sum\n",
    "    freq_T = freq_T / freq_sum\n",
    "    \n",
    "    meme_string = f\"\"\"MEME version 4\n",
    "    \n",
    "ALPHABET = ACGT\n",
    "\n",
    "strands: + -\n",
    "\n",
    "Background letter frequencies\n",
    "A {freq_A} C {freq_C} G {freq_G} T {freq_T} \n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "    for idx in range(len(pwms)):\n",
    "        identifier = f\"{idx + 1}\"\n",
    "        \n",
    "        try:\n",
    "            acgt_pwm = pwms[idx][[0, 1, 2, 4], :] / np.linalg.norm(pwms[idx][[0, 1, 2, 4], :], ord=1, axis=0)\n",
    "        \n",
    "            matrix = np.array2string(\n",
    "                acgt_pwm.T, \n",
    "                separator=\"  \", \n",
    "                edgeitems=np.inf, \n",
    "                suppress_small=True, \n",
    "                precision=5,\n",
    "                floatmode=\"fixed\",\n",
    "            ).replace(\"]\",\"\").replace(\"[[\", \"\").replace(\" [\",\"\")\n",
    "    \n",
    "            meme_string += f\"\"\"MOTIF {identifier}\n",
    "letter-probability matrix: alength= 4 w={window}\n",
    "{matrix}\n",
    "\n",
    "\"\"\"\n",
    "        except RuntimeWarning:\n",
    "            # Ignore motifs with nans caused by div-zero errors\n",
    "            continue\n",
    "        \n",
    "    return meme_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Filter Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_promoter_filters(filters, feature_maps, percentile_cutoff, selection=None, entropies=None):\n",
    "    NUM_ROWS = 8\n",
    "    NUM_COLS = 2\n",
    "    \n",
    "    num_filter, A, filter_len = filters.shape\n",
    "    if selection is None:\n",
    "        selection = [*range(num_filter)]\n",
    "    \n",
    "    fig, axs = plt.subplots(ncols=NUM_COLS * 2, nrows=NUM_ROWS, figsize=(11, 8.5), constrained_layout=True)\n",
    "    \n",
    "    for idx, (n, f) in enumerate(zip(selection, filters[selection])):\n",
    "        row = idx // NUM_COLS\n",
    "        col = idx % NUM_COLS\n",
    "            \n",
    "        # Calculate sequence logo heights -- information\n",
    "        # Excerpt from MEME Suite ceqlogo.c source code\n",
    "        \"\"\"\n",
    "        The letter stacks are calculated by the following equations found in\n",
    "        Schneider and Stephens paper \"Sequence Logos: A New Way to Display\n",
    "        Consensus Sequences\":\n",
    "\n",
    "         height = f(b,l) * R(l)                            (1)\n",
    "\n",
    "        where f(b,l) is the frequency of base or amino acid \"b\" at position\n",
    "        \"l\". R(l) is amount of information present at position \"l\" and can\n",
    "        be quantified as follows:\n",
    "\n",
    "         R(l) for amino acids   = log(20) - (H(l) + e(n))    (2a)\n",
    "         R(l) for nucleic acids =    2    - (H(l) + e(n))    (2b)\n",
    "\n",
    "        where log is taken base 2, H(l) is the uncertainty at position \"l\",\n",
    "        and e(n) is the error correction factor for small \"n\". H(l) is\n",
    "        computed as follows:\n",
    "\n",
    "           H(l) = - (Sum f(b,l) * log[ f(b,l) ])             (3)\n",
    "\n",
    "        where again, log is taken base 2. f(b,l) is the frequency of base\n",
    "        \"b\" at position \"l\". The sum is taken over all amino acids or\n",
    "        bases, depending on which the data is.\n",
    "\n",
    "        Currently, logo.pm uses an approximation for e(n), given by:\n",
    "\n",
    "           e(n) = (s-1) / (2 * ln2 * n)                      (4)\n",
    "\n",
    "        Where s is 4 for nucleotides, 20 for amino acids ; n is the number\n",
    "        of sequences in the alignment. e(n) also  gives the height of error\n",
    "        bars.\n",
    "        \"\"\"\n",
    "        e = 0 # No error correction\n",
    "        \n",
    "        H = -np.sum(f * np.log2(f+1e-7), axis=0, keepdims=True)\n",
    "        R = np.log2(5) - (H + e)\n",
    "        logo = f * R\n",
    "\n",
    "        # Create DataFrame for logomaker\n",
    "        counts_df = pd.DataFrame(data=0.0, columns=list('ACGNT'), index=list(range(9)))\n",
    "        for a in range(A):\n",
    "            for l in range(filter_len):\n",
    "                counts_df.iloc[l,a] = logo[a,l]\n",
    "\n",
    "        # Plot filter representation\n",
    "        logo = logomaker.Logo(counts_df, \n",
    "                              color_scheme=\"classic\",\n",
    "                              ax=axs[row, col * 2])\n",
    "        logo.style_spines(visible=False)\n",
    "        logo.style_spines(spines=[\"left\", \"bottom\"], visible=True)\n",
    "        logo.style_xticks(rotation=90, fmt='%d', anchor=0)\n",
    "\n",
    "        \n",
    "        logo.ax.xaxis.set_ticks_position('none')\n",
    "        logo.ax.xaxis.set_tick_params(pad=-1)\n",
    "        \n",
    "        axs[row, col * 2].set(title=f\"NC Motif #{n + 1}; Entropy: {entropies[idx]:.3f}\",\n",
    "                              xticks=[i for i in range(9)],\n",
    "                              xticklabels=[i + 1 for i in range(9)],\n",
    "                              ylim=(0, 2.2),\n",
    "                              ylabel=\"bits\")\n",
    "        \n",
    "        # Calculate plot of top activation distributions over sequence        \n",
    "        cutoff = np.percentile(feature_maps[:, n, :], percentile_cutoff)\n",
    "        transformed_fmap = feature_maps.copy()\n",
    "        transformed_fmap[transformed_fmap < cutoff] = np.nan\n",
    "        \n",
    "        # Also do a zero filter\n",
    "        transformed_fmap[transformed_fmap == 0] = np.nan\n",
    "\n",
    "        feature_length = feature_maps.shape[2]\n",
    "        x_range = [i for i in reversed(range(feature_length))]\n",
    "\n",
    "        counts = np.sum(~np.isnan(transformed_fmap[:, n, :]), axis=0)\n",
    "                \n",
    "        smoothk = 15\n",
    "        smoothed_counts = torch.nn.functional.avg_pool1d(\n",
    "            torch.from_numpy(counts).unsqueeze(0).unsqueeze(0),\n",
    "            kernel_size=(smoothk,),\n",
    "            stride=(1,),\n",
    "            padding=(smoothk//2,),\n",
    "            count_include_pad=False\n",
    "        ).view(-1).numpy()\n",
    "        \n",
    "        axs[row, col * 2 + 1].plot(x_range, smoothed_counts)\n",
    "        axs[row, col * 2 + 1].set(xlim=(0, feature_length), ylim=(0, 1.1 * np.max(smoothed_counts[10:feature_length-10])))\n",
    "        axs[row, col * 2 + 1].invert_xaxis()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 9\n",
    "c = 1\n",
    "percentile_cutoff = 99.5\n",
    "top_k = fmap.shape[0] * 5 # Top 0.5% of each filter\n",
    "\n",
    "promoter_pwms, promoter_entropies = get_pwms(x_test, fmap, window, c, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(promoter_pwms, f\"nc-elem-promoters-window{window}-top{top_k}-pwms.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = f\"nc-elem-promoters-window{window}-top{top_k}\"\n",
    "\n",
    "meme_string = get_MEME_string(promoter_pwms, frequencies, window)\n",
    "with open(f\"{prefix}-filter_pwm.txt\", \"w\") as f:\n",
    "    f.write(meme_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_PER_PAGE = 16\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "with PdfPages(f\"nc-elem-promoters-window{window}-smoothed-top{top_k}.pdf\") as pdf:\n",
    "    for page in range((len(promoter_pwms) + NUM_PER_PAGE - 1) // NUM_PER_PAGE):\n",
    "        start = page * NUM_PER_PAGE\n",
    "        end = min((page + 1) * NUM_PER_PAGE, len(promoter_pwms))\n",
    "        \n",
    "        filter_plots = plot_promoter_filters(\n",
    "            promoter_pwms, \n",
    "            fmap, \n",
    "            percentile_cutoff,\n",
    "            [*range(start, end)], \n",
    "            [promoter_entropies[i] for i in range(start, end)]\n",
    "        )\n",
    "        pdf.savefig(filter_plots)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
